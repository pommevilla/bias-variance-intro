<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paul Villanueva (github.com/pommevilla)">
<meta name="dcterms.date" content="2023-02-28">

<title>Bias and variance: a quick intro - A quick intro to bias and variance</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A quick intro to bias and variance</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Paul Villanueva (github.com/pommevilla) </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 28, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#a-typical-machine-learning-workflow" id="toc-a-typical-machine-learning-workflow" class="nav-link" data-scroll-target="#a-typical-machine-learning-workflow">A typical machine learning workflow</a></li>
  <li><a href="#bias-variance-under--and-overfitting" id="toc-bias-variance-under--and-overfitting" class="nav-link" data-scroll-target="#bias-variance-under--and-overfitting">Bias, variance, under- and overfitting</a>
  <ul class="collapse">
  <li><a href="#studying-for-a-math-test" id="toc-studying-for-a-math-test" class="nav-link" data-scroll-target="#studying-for-a-math-test">Studying for a math test</a></li>
  <li><a href="#the-bias-variance-tradeoff" id="toc-the-bias-variance-tradeoff" class="nav-link" data-scroll-target="#the-bias-variance-tradeoff">The bias-variance tradeoff</a></li>
  </ul></li>
  <li><a href="#a-hands-on-example" id="toc-a-hands-on-example" class="nav-link" data-scroll-target="#a-hands-on-example">A hands-on example</a>
  <ul class="collapse">
  <li><a href="#some-things-to-think-about" id="toc-some-things-to-think-about" class="nav-link" data-scroll-target="#some-things-to-think-about">Some things to think about</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="cell">

</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>I wrote this up because I wanted a pretty straightforward primer on the bias-variance trade off that was reasonably accessible to point people towards<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. There’s a sea of excellent material on the topic, but I’ve found that viewing the subject from a handful of different perspectives is what really gets me past the initial bump when learning a new topic. To that end, this page is intended to be a pretty high-level intro to the bias-variance tradeoff for people who are relatively new to machine learning. It’s not the be-all end-all discussion of the topic, but it is (I think) a decent exposure to the topic.</p>
<p>The purpose of this page is to:</p>
<ul>
<li>Introduce you to bias, variance, and some other concepts you’ll hear when learning about them with some concrete examples</li>
<li>Give you a hands-on example to play around with</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Assumptions
</div>
</div>
<div class="callout-body-container callout-body">
<p>While this article is intended for beginners, there are some light requirements to get the most out of it:</p>
<ul>
<li>You’ve trained a model before and have used it to make predictions.</li>
<li>You know what a training and testing set are.</li>
<li>You know how to evaluate a model’s performance using some metric (like accuracy, for example).</li>
</ul>
</div>
</div>
</section>
<section id="a-typical-machine-learning-workflow" class="level1">
<h1>A typical machine learning workflow</h1>
<p>In a typical prediction project, you are given a dataset and tasked with creating a model that can predict some label or output <code>y</code> from the set of features <code>X</code>. A (vastly) simplified<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> workflow might look like:</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">flowchart TB
    RawData[Raw Data]
    EDA[Exploratory Data Analysis]
    ModelTraining[Model training]
    ModelEvaluation[Model Evaluation]
    DeployModel[Use model]
    Question[Was model performance satisfactory?]

    RawData --&gt; EDA --&gt; ModelTraining --&gt; ModelEvaluation

    ModelEvaluation --&gt; Question
    Question-- No - repeat to\nimprove performance --&gt;EDA
    Question-- Yes --&gt; DeployModel

</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: A high-level overview of the machine learning process.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>During the model training step, the model tunes its parameters to optimize performance on the training set. Afterwards, we evaluate the model’s ability to perform on new data by using it to make predictions on the testing set. It is during these steps that we are considering bias and variance.</p>
</section>
<section id="bias-variance-under--and-overfitting" class="level1">
<h1>Bias, variance, under- and overfitting</h1>
<p>Bias and variance are used to describe a model’s performance on training and testing sets. Put simply, <em>bias</em> describes the error in the model on the training set, while <em>variance</em> describes the error in the model when exposed to the testing set.</p>
<p>The concepts of model <em>underfitting</em> or <em>overfitting</em> to training data are different lenses through which we can understand bias and variance. There are many ways to think about underfitting and overfitting.</p>
<ul>
<li>In terms of model performance, <em>underfitting</em> occurs when a model doesn’t perform well on the training set, and <em>overfitting</em> occurs when the model performs very well on the training set but poorly on the testing set.</li>
<li>More abstractly, underfitting is what happens when a model is too simple to explain the nature of the data, while overfitting is when the model doesn’t separate the “signal” in the data from “noise” and incorrectly focuses on that noise as important<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</li>
<li>In terms of bias and variance, models that underfit the data have high bias and low variance, while models that overfit have low bias and high variance.</li>
</ul>
<p><a href="#fig-all-fit-plots">Figure&nbsp;2</a> shows examples of these situations.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-all-fit-plots" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-all-fit-plots-1.png" class="img-fluid figure-img" alt="Three plots showing models that underfit, overfit, and fit well to training data." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Models that A) underfit, B) overfit, and C) fit well to the training data (blue).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In the figure, the black dashed line is trained on the purple dots, while the yellow dots indicate points held out for testing. The black dashed line is a polynomial equation of degree <span class="math inline">\(n\)</span>. Since the data obviously has some non-linear component to it, we test polynomials of different degrees to find a good fit to the data.</p>
<ul>
<li>Plot A is an example of a model that underfits the data. This is a polynomial of degree 1, a line, and is not complex enough to capture the behavior of the data. We say that it has relatively <em>high bias</em> because its predictions on the training set aren’t very good, but has relatively <em>low variance</em> because its predictions won’t get much worse than what they are on the training set.</li>
<li>Plot B, on the other hand, is overfitting the data with an overly complex model. This is a polynomial of degree 9, which enables it to accurately predict all of the training points. However, its predictions for the testing set are generally bad. This model has <em>low bias</em> because of how well it performs in training, but extremely <em>high variance</em> due to how badly it performs on new data<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>Plot C is a good middle ground between the two previous cases. It does have some bias as it does not perform amazingly well at prediction on the training set, but it is able to more adequately capture the relationship in the data without “memorizing” its shape (lower variance).</li>
</ul>
<section id="studying-for-a-math-test" class="level2">
<h2 class="anchored" data-anchor-id="studying-for-a-math-test">Studying for a math test</h2>
<p>Here’s an example that helped me understand this stuff. Pretend that you have a math exam next week and you’re trying to figure out how to prepare for it. Your friend was able to get all of the previous year’s exams for the same class and has shared it with you. Here are three possible scenarios:</p>
<ol type="1">
<li>You don’t look at the practice tests, you don’t study, and you don’t do practice problems. You show up to the exam and completely bomb because of course you do.</li>
<li>All you do is study the practice tests. You do the problems over and over until you can get them 100% correct without trying. On the day of the exam, the questions are completely different because of course they are and you fail.</li>
<li>You take a look at the practice tests to get a general idea of what kind of questions the professor asks on the exam for this material. You study that material in the book and look over recent homework questions covering those topics. The exam is still hard and the questions are different, but you’re prepared to handle new questions because you’re a good little student.</li>
</ol>
<p>Now that we are equipped with these new terms, we see that scenario one describes the scenario where you underfit the material while you overfit the material in the second scenario. In the third scenario, rather than learn the material, you learn it well enough to adapt to new questions<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
</section>
<section id="the-bias-variance-tradeoff" class="level2">
<h2 class="anchored" data-anchor-id="the-bias-variance-tradeoff">The bias-variance tradeoff</h2>
<p>In a perfect world, we would have a model with low bias that performs amazingly on the training set, but also has low variance and is able to generalize to any new data we throw at it. Unfortunately, that is simply not the case, and all models fall on somewhere on this spectrum:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-2">flowchart LR
    A[Low bias\nHigh variance]
    B[High bias\nLow variance]

    A &lt;--&gt; B
</pre>
<div id="mermaid-tooltip-2" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
<p>Models with low bias and high variance (ie, overfit the data) are on one end and models with high bias and low variance (underfit) are on the other end. The main task in the model training process is to find the model that has an acceptable level of bias and variance for your task.</p>
<p>However, it is impossible to completely eliminate both bias and variance. To understand why this is true, let’s begin with a model that has high bias and low variance. The model has high bias and low variance because it is too simple to explain the data. This may be because we are not using all of the variables avaiable to use, or maybe because the model itself isn’t complex enough. In either case, we introduce more complexity to the model and it begins to fit better to the training set. However, at the same time that its bias is decreasing, its variance is naturally increasing because the newly acquired model complexity is causing it to fit more strongly to the training data, making it (generally) perform a bit worse on new data.</p>
<p>On the other hand, a complicated model with low bias but high variance can improve by simplifying itself. For example, in the plots above, decreasing the degree of the polynomial decreases the complexity of the model. Relaxing that complexity and decreasing the variance, though, comes at the cast of increased prediction errors on the training set.</p>
<p>This tension between bias and variance, known as the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias-variance tradeoff</a> is a central conflict in machine learning, and much of the machine learning process is spent tackling this issue. Two of the ways the bias-variance tradeoff is addressed is through <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross validation</a> and evening out <a href="imbalanced data">https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data</a>,</p>
</section>
</section>
<section id="a-hands-on-example" class="level1">
<h1>A hands-on example</h1>
<p>I put together this small Shiny app so you can see these concepts at work. The elements of the app are:</p>
<ul>
<li>Some semi-random data points<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> separated into a training (blue points) and testing set (red points). The default split of the data is 70% training and 30% testing points (a common training-test split). You are able to vary the proportion of points that belong to the training set by dragging the “Training set proportion” slider.</li>
<li>The blue line is a polynomial fit to the <em>training</em> data. You can vary the degree of the polynomial by dragging the slider - higher degree polynomials increase the complexity of the model, allowing it to better fit to the training data.</li>
<li>Below the graph are the <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">root-mean-square errors (RMSE)</a> for the polynomial against the training set and the testing set - the better the model performs, the lower the RMSE will be.</li>
<li>The difference between the training and testing RMSE is also indicated: a negative difference indicates that the training RMSE was lower than the testing RMSE.</li>
</ul>
<div class="shiny-iframe">
    <iframe width="100%" height="700" src="https://pommevilla.shinyapps.io/bias-variance-dashboard/">
    </iframe>
</div>
<section id="some-things-to-think-about" class="level2">
<h2 class="anchored" data-anchor-id="some-things-to-think-about">Some things to think about</h2>
<p>Here are some things to consider as you vary the model complexity and training set proportion of the data:</p>
<ol type="1">
<li>Keep track of how the RMSE changes with the degree - for which degrees does the model perform best? How are you making this decision?</li>
<li>If you had to choose a degree polynomial as the “best fit,” which one would you choose? Why?</li>
<li>The training RMSE of the degree 9 polynomial is 329.96, while it is 319.32 for degree 10. If you <em>had</em> to pick between these two models, which one would you pick? Why?</li>
<li>The degree of the polynomial is a <a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">hyperparameter</a>, How are hyperparameters different than parameters?</li>
<li>The process of picking the degree that performs best is called <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyperparameter tuning (or optimization)</a>. We’re using the app to do this manually, but how would you do this process in a real machine learning project?</li>
<li>Keeping the degree the same, begin varying the training set proportion. How does this effect the various RMSE metrics? How can you explain this in terms of bias and variance?</li>
</ol>
</section>
</section>
<section id="next-steps" class="level1">
<h1>Next steps</h1>
<p>Here are some links</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The original audience for this was the students in <a href="https://github.com/isu-abe/516x">ABE 516x</a>, the ABE department at ISU’s introduction to data science research methods. Also, I’m procrastinating on writing my dissertation.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I say simplified because a) pretty much every box in this diagram is its own topic that requires consideration b) there is lot more back-and-forth between steps in practice. <a href="https://archive-beta.ics.uci.edu/">UCI Machine Learning Repository</a> or the <a href="https://github.com/rfordatascience/tidytuesday">Tidy Tuesday project</a>)<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>You can conceptualize the distribution of some data set containing <code>x</code> and <code>y</code> points by some function <span class="math inline">\(y = f(x) + \varepsilon(x)\)</span>, where <span class="math inline">\(f(x)\)</span> is the “true” relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and <span class="math inline">\(\varepsilon\)</span> is some error term. Machine learning in general tries to learn some <span class="math inline">\(\hat{f}\)</span> to approximate <span class="math inline">\(f\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Another aspect to consider is the deliverable of a machine learning project. Typically this is to produce some model so that future data can be used in prediction, but another common expectation from a model is some sort of insight into what the relationship between the inputs and outputs are. While linear models are simple and often don’t capture the entirety of the relationship in a dataset, it does well enough, and more importantly, is explainable - given some inputs, you can see exactly how the model comes to its decisions. When using more complicated models, gains in model performance often come at the cost of human interpretability, usually to the point where we can’t explain how a model decides what it does (the so-called <a href="https://www.nature.com/articles/d41586-022-00858-1">black box problem</a>).<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Like most examples that try to explain abstract, mathematical concepts, this one falls apart pretty quickly when you start asking questions (“Math exams are data sets? I’m the model?”), but just go with it.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>I randomly generated some points <code>x</code> via <code>runif(75, 1, 20)</code>, then mapped them to <code>f(x) = x^2 / 20 + runif(1, 0, 25)</code><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>