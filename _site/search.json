[
  {
    "objectID": "index.html#studying-for-a-math-test",
    "href": "index.html#studying-for-a-math-test",
    "title": "A quick intro to bias and variance",
    "section": "Studying for a math test",
    "text": "Studying for a math test\nHere’s an example that helped me understand this stuff. Pretend that you have a math exam next week and you’re trying to figure out how to prepare for it. Your friend was able to get all of the previous year’s exams for the same class and has shared it with you. Here are three possible scenarios:\n\nYou don’t look at the practice tests, you don’t study, and you don’t do practice problems. You show up to the exam and completely bomb because of course you do.\nAll you do is study the practice tests. You do the problems over and over until you can get them 100% correct without trying. On the day of the exam, the questions are completely different because of course they are and you fail.\nYou take a look at the practice tests to get a general idea of what kind of questions the professor asks on the exam for this material. You study that material in the book and look over recent homework questions covering those topics. The exam is still hard and the questions are different, but you’re prepared to handle new questions because you’re a good little student.\n\nNow that we are equipped with these new terms, we see that scenario one describes the scenario where you underfit the material while you overfit the material in the second scenario. In the third scenario, rather than learn the material, you learn it well enough to adapt to new questions5."
  },
  {
    "objectID": "index.html#the-bias-variance-tradeoff",
    "href": "index.html#the-bias-variance-tradeoff",
    "title": "A quick intro to bias and variance",
    "section": "The bias-variance tradeoff",
    "text": "The bias-variance tradeoff\nIn a perfect world, we would have a model with low bias that performs amazingly on the training set, but also has low variance and is able to generalize to any new data we throw at it. Unfortunately, that is simply not the case, and all models fall on somewhere on this spectrum:\n\n\n\n\nflowchart LR\n    A[Low bias\\nHigh variance]\n    B[High bias\\nLow variance]\n\n    A <--> B\n\n\n\n\n\n\n\n\nModels with low bias and high variance (ie, overfit the data) are on one end and models with high bias and low variance (underfit) are on the other end. The main task in the model training process is to find the model that has an acceptable level of bias and variance for your task.\nHowever, it is impossible to completely eliminate both bias and variance. To understand why this is true, let’s begin with a model that has high bias and low variance. The model has high bias and low variance because it is too simple to explain the data. This may be because we are not using all of the variables avaiable to use, or maybe because the model itself isn’t complex enough. In either case, we introduce more complexity to the model and it begins to fit better to the training set. However, at the same time that its bias is decreasing, its variance is naturally increasing because the newly acquired model complexity is causing it to fit more strongly to the training data, making it (generally) perform a bit worse on new data.\nOn the other hand, a complicated model with low bias but high variance can improve by simplifying itself. For example, in the plots above, decreasing the degree of the polynomial decreases the complexity of the model. Relaxing that complexity and decreasing the variance, though, comes at the cast of increased prediction errors on the training set.\nThis tension between bias and variance, known as the bias-variance tradeoff is a central conflict in machine learning, and much of the machine learning process is spent tackling this issue. Two of the ways the bias-variance tradeoff is addressed is through cross validation and evening out https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data,"
  },
  {
    "objectID": "index.html#some-things-to-think-about",
    "href": "index.html#some-things-to-think-about",
    "title": "A quick intro to bias and variance",
    "section": "Some things to think about",
    "text": "Some things to think about\nHere are some things to consider as you vary the model complexity and training set proportion of the data:\n\nKeep track of how the RMSE changes with the degree - for which degrees does the model perform best? How are you making this decision?\nIf you had to choose a degree polynomial as the “best fit,” which one would you choose? Why?\nThe training RMSE of the degree 9 polynomial is 329.96, while it is 319.32 for degree 10. If you had to pick between these two models, which one would you pick? Why?\nThe degree of the polynomial is a hyperparameter, How are hyperparameters different than parameters?\nThe process of picking the degree that performs best is called hyperparameter tuning (or optimization). We’re using the app to do this manually, but how would you do this process in a real machine learning project?\nKeeping the degree the same, begin varying the training set proportion. How does this effect the various RMSE metrics? How can you explain this in terms of bias and variance?"
  }
]